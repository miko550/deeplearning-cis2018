{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Intrusion Detection System (IDS)\n",
    "## Complete Project Notebook\n",
    "\n",
    "**Project**: Network Traffic Classification using Deep Learning  \n",
    "**Dataset**: CICIDS2018 Network Traffic Data  \n",
    "**Objective**: Classify network traffic into different attack types and benign traffic\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Problem Statement](#problem-statement)\n",
    "2. [Data Loading and Exploration](#data-loading)\n",
    "3. [Exploratory Data Analysis (EDA)](#eda)\n",
    "4. [Data Preprocessing](#preprocessing)\n",
    "5. [Model Architecture Selection](#model-architecture)\n",
    "6. [Model Training](#training)\n",
    "7. [Results and Evaluation](#results)\n",
    "8. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement {#problem-statement}\n",
    "\n",
    "### What are we trying to solve?\n",
    "\n",
    "**Problem**: Network security threats are increasing, and traditional signature-based intrusion detection systems struggle with:\n",
    "- Zero-day attacks (unknown attack patterns)\n",
    "- Encrypted traffic\n",
    "- High-volume network traffic\n",
    "- Evolving attack techniques\n",
    "\n",
    "**Solution**: Develop a Deep Learning-based Intrusion Detection System that can:\n",
    "- Automatically learn patterns from network traffic features\n",
    "- Classify traffic into multiple attack categories\n",
    "- Detect both known and unknown attack patterns\n",
    "- Handle high-dimensional feature spaces\n",
    "\n",
    "**Dataset**: CICIDS2018 - Contains network traffic flows with labeled attack types:\n",
    "- Benign traffic\n",
    "- Various attack types (DDoS, Brute Force, Infiltration, etc.)\n",
    "\n",
    "**Goal**: Build a multi-class classifier that can accurately identify different types of network attacks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate_model' from 'test' (/usr/lib/python3.13/test/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpreprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model, list_available_models\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test_and_report, evaluate_model\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_class_weights, analyze_class_distribution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deeplearning-cis2018/train.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model\u001b[39m(model, train_loader, val_loader, device, criterion, optimizer, scheduler, num_epochs):\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    Trains the model for a given number of epochs.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \u001b[33;03m        The trained model.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'evaluate_model' from 'test' (/usr/lib/python3.13/test/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Fix: Ensure local test.py is used instead of standard library test module\n",
    "import sys\n",
    "import os\n",
    "# Add current directory to path if not already there\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Custom modules\n",
    "from preprocess import preprocess\n",
    "from models import create_model, list_available_models\n",
    "from train import train_model\n",
    "from test import test_and_report, evaluate_model\n",
    "from utils import calculate_class_weights, analyze_class_distribution\n",
    "\n",
    "print(\"\u2713 All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Exploration {#data-loading}\n",
    "\n",
    "Let's load the preprocessed data and explore its characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {config.get('model_name', 'mlp')}\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "print(\"Loading data...\")\n",
    "train = np.load('data/train.npy')\n",
    "test = np.load('data/test.npy')\n",
    "val = np.load('data/val.npy')\n",
    "class_names = np.load('data/class_names.npy', allow_pickle=True)\n",
    "class_names = [str(name) for name in class_names]\n",
    "\n",
    "print(f\"\\n\u2713 Data loaded successfully!\")\n",
    "print(f\"  Train shape: {train.shape}\")\n",
    "print(f\"  Test shape: {test.shape}\")\n",
    "print(f\"  Val shape: {val.shape}\")\n",
    "print(f\"  Number of features: {train.shape[1] - 1}\")\n",
    "print(f\"  Number of classes: {len(class_names)}\")\n",
    "print(f\"\\n  Classes: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA) {#eda}\n",
    "\n",
    "Let's analyze the dataset to understand its characteristics, class distribution, and potential challenges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels from each dataset\n",
    "train_labels = train[:, -1].astype(int)\n",
    "val_labels = val[:, -1].astype(int)\n",
    "test_labels = test[:, -1].astype(int)\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"=\"*70)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, labels in [('TRAIN', train_labels), ('VALIDATION', val_labels), ('TEST', test_labels)]:\n",
    "    print(f\"\\n{name} SET:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = len(labels)\n",
    "    \n",
    "    for class_idx, count in zip(unique, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        class_name = class_names[class_idx] if class_idx < len(class_names) else f\"Class {class_idx}\"\n",
    "        print(f\"  Class {class_idx} ({class_name:30s}): {count:8,} samples ({percentage:6.2f}%)\")\n",
    "    \n",
    "    print(f\"  Total: {total:,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, labels) in enumerate([('Train', train_labels), \n",
    "                                        ('Validation', val_labels), \n",
    "                                        ('Test', test_labels)]):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    class_labels = [class_names[i] if i < len(class_names) else f\"Class {i}\" for i in unique]\n",
    "    \n",
    "    axes[idx].bar(range(len(unique)), counts, color=plt.cm.Set3(range(len(unique))))\n",
    "    axes[idx].set_title(f'{name} Set Class Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Class')\n",
    "    axes[idx].set_ylabel('Number of Samples')\n",
    "    axes[idx].set_xticks(range(len(unique)))\n",
    "    axes[idx].set_xticklabels([f'{i}\\n{name[:15]}' for i, name in zip(unique, class_labels)], \n",
    "                              rotation=45, ha='right', fontsize=8)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        axes[idx].text(i, count, f'{count:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Class distribution visualization saved as 'class_distribution.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class imbalance ratio\n",
    "train_unique, train_counts = np.unique(train_labels, return_counts=True)\n",
    "max_count = train_counts.max()\n",
    "min_count = train_counts.min()\n",
    "imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "\n",
    "print(f\"\\nClass Imbalance Analysis:\")\n",
    "print(f\"  Maximum class count: {max_count:,}\")\n",
    "print(f\"  Minimum class count: {min_count:,}\")\n",
    "print(f\"  Imbalance ratio: {imbalance_ratio:.2f}x\")\n",
    "print(f\"\\n  Interpretation:\")\n",
    "if imbalance_ratio > 100:\n",
    "    print(f\"    \u26a0 Severe class imbalance - Consider class weights or resampling\")\n",
    "elif imbalance_ratio > 10:\n",
    "    print(f\"    \u26a0 Moderate class imbalance - Class weights recommended\")\n",
    "else:\n",
    "    print(f\"    \u2713 Relatively balanced dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature statistics\n",
    "train_features = train[:, :-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFeature shape: {train_features.shape}\")\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(f\"  Mean: {train_features.mean():.4f}\")\n",
    "print(f\"  Std:  {train_features.std():.4f}\")\n",
    "print(f\"  Min:  {train_features.min():.4f}\")\n",
    "print(f\"  Max:  {train_features.max():.4f}\")\n",
    "\n",
    "# Check for any remaining NaN or Inf\n",
    "nan_count = np.isnan(train_features).sum()\n",
    "inf_count = np.isinf(train_features).sum()\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"  NaN values: {nan_count}\")\n",
    "print(f\"  Inf values: {inf_count}\")\n",
    "\n",
    "if nan_count == 0 and inf_count == 0:\n",
    "    print(f\"  \u2713 Data is clean (no NaN or Inf values)\")\n",
    "else:\n",
    "    print(f\"  \u26a0 Data contains NaN or Inf values - needs cleaning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing {#preprocessing}\n",
    "\n",
    "Now let's preprocess the data: standardize features and create DataLoaders for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Preprocess data and create DataLoaders\n",
    "print(\"\\nPreprocessing data...\")\n",
    "train_loader, test_loader, val_loader = preprocess(\n",
    "    train, test, val,\n",
    "    batch_size=config['batch_size'],\n",
    "    scaler_save_path='scaler.pkl'\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Data preprocessing complete!\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Selection {#model-architecture}\n",
    "\n",
    "We'll test multiple architectures to find the best one. Let's start by examining available models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "available_models = list_available_models()\n",
    "print(\"Available Model Architectures:\")\n",
    "print(\"=\"*70)\n",
    "for i, model_name in enumerate(available_models, 1):\n",
    "    print(f\"{i}. {model_name.upper()}\")\n",
    "\n",
    "# Get model configuration\n",
    "model_name = config.get('model_name', 'mlp')\n",
    "model_params = config.get('model_params', {})\n",
    "\n",
    "print(f\"\\nSelected Model: {model_name.upper()}\")\n",
    "if model_params:\n",
    "    print(f\"Model Parameters: {model_params}\")\n",
    "else:\n",
    "    print(\"Using default model parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "num_features = train.shape[1] - 1\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = create_model(\n",
    "    model_name=model_name,\n",
    "    input_features=num_features,\n",
    "    num_classes=num_classes,\n",
    "    **model_params\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(\"=\"*70)\n",
    "summary(model, input_size=(num_features,), device=device)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Justification\n",
    "\n",
    "**Why this architecture?**\n",
    "\n",
    "1. **MLP (Multi-Layer Perceptron)**: \n",
    "   - Simple feedforward network\n",
    "   - Good baseline for tabular data\n",
    "   - Fast training and inference\n",
    "   - Effective for high-dimensional feature spaces\n",
    "\n",
    "2. **CNN (Convolutional Neural Network)**:\n",
    "   - Captures local patterns in features\n",
    "   - 1D convolutions work well for sequential-like data\n",
    "   - Global pooling reduces overfitting\n",
    "\n",
    "3. **LSTM/GRU**:\n",
    "   - Models sequential dependencies\n",
    "   - Handles long-term patterns\n",
    "   - Good if features have temporal relationships\n",
    "\n",
    "4. **Transformer**:\n",
    "   - Attention mechanism captures feature relationships\n",
    "   - State-of-the-art performance\n",
    "   - Parallel processing\n",
    "\n",
    "**Our Choice**: Starting with MLP as baseline, then comparing with other architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training {#training}\n",
    "\n",
    "Set up training components and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "train_labels = train[:, -1].astype(int)\n",
    "class_weights = calculate_class_weights(train_labels, method='balanced')\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Setup loss function with class weights\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "print(f\"\\nLoss function: Weighted CrossEntropyLoss\")\n",
    "print(f\"  Using class weights to handle imbalanced dataset\")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3\n",
    ")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Reduces LR by 10x when validation loss stops improving\")\n",
    "print(f\"  Patience: 3 epochs\")\n",
    "\n",
    "print(f\"\\n\u2713 Training components setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb for experiment tracking\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"{model_name}_{timestamp}_bs{config['batch_size']}_lr{config['learning_rate']}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"DL-CIS2018\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"batch_size\": config['batch_size'],\n",
    "        \"num_epochs\": config['num_epochs'],\n",
    "        \"learning_rate\": config['learning_rate'],\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": \"ReduceLROnPlateau\",\n",
    "        **model_params\n",
    "    }\n",
    ")\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "print(f\"\u2713 Wandb initialized\")\n",
    "print(f\"  Project: DL-CIS2018\")\n",
    "print(f\"  Run name: {run_name}\")\n",
    "print(f\"  View at: https://wandb.ai\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING PHASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store training history for visualization\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=config['num_epochs']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2713 Training complete!\")\n",
    "print(f\"\u2713 Best model saved to: DL-CIS2018.pth\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results and Evaluation {#results}\n",
    "\n",
    "Now let's evaluate the trained model on the test set and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING PHASE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_accuracy = test_and_report(\n",
    "    model=trained_model,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\u2713 Final Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed predictions for confusion matrix visualization\n",
    "trained_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for samples, labels in test_loader:\n",
    "        samples = samples.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        \n",
    "        predictions = trained_model(samples)\n",
    "        preds = torch.argmax(predictions, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Confusion matrix saved as 'confusion_matrix.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, labels=range(len(class_names)), zero_division=0\n",
    ")\n",
    "\n",
    "# Create metrics dataframe\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance Metrics:\")\n",
    "print(\"=\"*70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize per-class metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "x_pos = np.arange(len(class_names))\n",
    "width = 0.6\n",
    "\n",
    "axes[0].bar(x_pos, precision, width, color='skyblue', alpha=0.8)\n",
    "axes[0].set_title('Precision per Class', fontweight='bold')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([f'{i}' for i in range(len(class_names))], rotation=45)\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(x_pos, recall, width, color='lightcoral', alpha=0.8)\n",
    "axes[1].set_title('Recall per Class', fontweight='bold')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([f'{i}' for i in range(len(class_names))], rotation=45)\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[2].bar(x_pos, f1, width, color='lightgreen', alpha=0.8)\n",
    "axes[2].set_title('F1-Score per Class', fontweight='bold')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('F1-Score')\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels([f'{i}' for i in range(len(class_names))], rotation=45)\n",
    "axes[2].set_ylim([0, 1.1])\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Per-class metrics visualization saved as 'per_class_metrics.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion {#conclusion}\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "Let's summarize what we learned from this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total training samples: {len(train):,}\")\n",
    "print(f\"  Total validation samples: {len(val):,}\")\n",
    "print(f\"  Total test samples: {len(test):,}\")\n",
    "print(f\"  Number of features: {num_features}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "\n",
    "print(f\"\\nModel:\")\n",
    "print(f\"  Architecture: {model_name.upper()}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Loss: Weighted CrossEntropyLoss\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Best model saved: DL-CIS2018.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Worked?\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - StandardScaler normalization improved training stability\n",
    "   - Class weights helped handle imbalanced dataset\n",
    "   - Proper train/val/test split ensured fair evaluation\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - [Your model] performed well with [specific metrics]\n",
    "   - BatchNorm and Dropout prevented overfitting\n",
    "   - Learning rate scheduling improved convergence\n",
    "\n",
    "3. **Training Strategy**:\n",
    "   - Early stopping prevented overfitting\n",
    "   - Gradient clipping stabilized training\n",
    "   - Weighted loss handled class imbalance\n",
    "\n",
    "### What Didn't Work?\n",
    "\n",
    "1. **Challenges Encountered**:\n",
    "   - [List any issues: e.g., rare classes, overfitting, etc.]\n",
    "\n",
    "2. **Areas for Improvement**:\n",
    "   - Could try different architectures (CNN, LSTM, Transformer)\n",
    "   - Hyperparameter tuning could improve performance\n",
    "   - Data augmentation might help with rare classes\n",
    "   - Ensemble methods could boost accuracy\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Class Imbalance**: The dataset has significant class imbalance, requiring class weights\n",
    "2. **Feature Engineering**: Network traffic features are well-suited for deep learning\n",
    "3. **Model Selection**: [Your model] achieved [X]% accuracy, which is [good/excellent] for this task\n",
    "4. **Scalability**: The model can handle large-scale network traffic classification\n",
    "\n",
    "### Future Work\n",
    "\n",
    "1. **Model Improvements**:\n",
    "   - Experiment with different architectures\n",
    "   - Try ensemble methods\n",
    "   - Implement attention mechanisms\n",
    "\n",
    "2. **Data Improvements**:\n",
    "   - Collect more data for rare classes\n",
    "   - Feature engineering and selection\n",
    "   - Data augmentation techniques\n",
    "\n",
    "3. **Deployment**:\n",
    "   - Real-time inference optimization\n",
    "   - Model compression for edge devices\n",
    "   - Integration with network monitoring systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Notebook\n",
    "\n",
    "**Project Completed**: Deep Learning-based Intrusion Detection System\n",
    "\n",
    "**Final Accuracy**: [Your accuracy]%\n",
    "\n",
    "**Model Saved**: DL-CIS2018.pth\n",
    "\n",
    "**Next Steps**: \n",
    "- Try different architectures using `compare_models.py`\n",
    "- Tune hyperparameters for better performance\n",
    "- Deploy model for real-world use\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook can be run from top to bottom. Make sure you have:*\n",
    "- *Preprocessed data files (train.npy, test.npy, val.npy, class_names.npy)*\n",
    "- *All required Python packages installed*\n",
    "- *Wandb account configured (optional)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss Visualization\n",
    "\n",
    "If you have wandb logged, you can visualize training curves. Alternatively, we can extract from training logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: To visualize training curves from wandb, you can:\n",
    "# 1. View them directly in wandb dashboard at https://wandb.ai\n",
    "# 2. Or use wandb API to fetch and plot:\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"DL-CIS2018/{run_name}\")\n",
    "    history = run.history()\n",
    "    \n",
    "    if 'Train Loss' in history.columns and 'Val Loss' in history.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['Train Loss'], label='Train Loss', linewidth=2)\n",
    "        plt.plot(history['Val Loss'], label='Val Loss', linewidth=2)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss', fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        if 'Learning Rate' in history.columns:\n",
    "            plt.plot(history['Learning Rate'], label='Learning Rate', color='green', linewidth=2)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.title('Learning Rate Schedule', fontweight='bold')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"\u2713 Training curves saved as 'training_curves.png'\")\n",
    "    else:\n",
    "        print(\"\u26a0 Training history not available. Check wandb dashboard for visualizations.\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0 Could not fetch wandb data: {e}\")\n",
    "    print(\"  View training curves at: https://wandb.ai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison (Optional)\n",
    "\n",
    "If you want to compare multiple models, you can run the comparison script or test different architectures here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Quick comparison of a few models\n",
    "# Uncomment to run (this will take time)\n",
    "\n",
    "\"\"\"\n",
    "models_to_test = ['mlp', 'cnn', 'lstm']\n",
    "comparison_results = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nTesting {model_name.upper()}...\")\n",
    "    # Create and train model (simplified - use compare_models.py for full comparison)\n",
    "    # ... training code ...\n",
    "    pass\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}